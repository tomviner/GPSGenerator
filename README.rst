GPS tracking simulation for a Courier-Workers coordination
==========================================================

Introduction
-----------------
This a small simulator for generating gps coordinates, simulating a fleet of couriers or workers within big cities.

Objective
-------------
The main objective for this script is to have a flexible, easy to use and customizable stream of gps corrdinates.

Motivation
---------------
I need this tool for testing database storing of a big stream of gps tracking.
This is for personal use. I am considering transforming this repo in a small library agnostic to databases and personal data structures.

Proof of concept
------------------
Why store raw gps tracks inline if we can process them with some structure?
Structure the data first! This is the main goal. It will allow us to gain two very important goals:

> **Release-the-database:**  With the purpose of not hitting the database for writing, even if there is a write only master (postgres) lets design something that will allow us to scale the system from the beginning.
The intention of this is to save in a redis server de data flow generated by the couriers. Similar to a buffer. And collect them in a bulk process (worker, for example celery) when the monitoring system says it is better.

> **Storing some structured data in memory(redis):**  This will give us some realtime, observable state. ( ok realtime is an illusion in networked architectures like this ) This means we can query the redis about active deliveries, visualize them in a frontend(map) and track the real situation and location of the couriers without hitting the database.

**possible disadvantages**

-  Delaying disk writes in the database could be tricky, doing it in a bulk process can cause an overhead in disk writes. It is necessary to schedule this task in many small steps. Doing this in small hours could recommendable. It is possible that already exists a redis-potgresql syncing solution or using redis as a cache layer for postgres. .. (I need further study)
·· Interesting link: https://github.com/pg-redis-fdw/redis_fdw

-  Redis itself: . If the server dies or there is a network problem, storing a big dataflow in redis could be dangerous. We can have issues with data integrity. A Master-slave replication with a falldown detection could be the solution. Here, the key is the correct shceduling of workers that will collect the data to de database.



Implementation specifics
--------------------------


Why not TDD
-------------------
I thought that python generators, coroutines and a scheduler were the most apropiate solution, for solving the problem. However, I really did not know how deep this rabbithole could go so, I decided not to do tdd this time. This script is a a proof of concept. I will rebuild this with tests, when i really know how to do it.

Performance Issues
--------------------------- 
I wanted a small memory footprint script, that is why I decided to do it with generators. On the one hand, the memory use is amazingly low. On the other hand, the cpu use is very intensive, something very normal in this kind of scripts(python).
Edit** I found something remarkable, once coroutines are created, and testing it with a contant flow of generated data. Memory footprint of the all machinery goes stable, constant in 443,76MBytes, due to the big dataflow stream is generating, this is beyond all my expectations!

Speed Issues
------------------
This script generates a really big stream. I decided to implemente this generator and scheduler trick, to simulate some "concurrency", and have thorough control of the simulation, in one thread. That is why this script goes slow.

